{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worksheet 11\n",
    "\n",
    "Name:  Thian Amarasekera\n",
    "\n",
    "UID: U91933393\n",
    "\n",
    "### Topics\n",
    "\n",
    "- Latent Semantic Analysis\n",
    "\n",
    "### Latent Semantic Analysis\n",
    "\n",
    "In this section we will fetch news articles from 3 different categories. We will perform Tfidf vectorization on the corpus of documents and use SVD to represent our corpus in the feature space of topics that we've uncovered from SVD. We will attempt to cluster the documents into 3 clusters as we vary the number of singular vectors we use to represent the corpus, and compare the output to the clustering created by the news article categories. Do we end up with a better clustering the more singular vectors we use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/thian/nltk_data'\n    - '/Users/thian/opt/anaconda3/nltk_data'\n    - '/Users/thian/opt/anaconda3/share/nltk_data'\n    - '/Users/thian/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/thian/Desktop/Data-Science-Fundamentals/lecture_11/worksheet_11.ipynb Cell 2\u001b[0m line \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thian/Desktop/Data-Science-Fundamentals/lecture_11/worksheet_11.ipynb#W1sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m news_data \u001b[39m=\u001b[39m fetch_20newsgroups(subset\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m, categories\u001b[39m=\u001b[39mcategories)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thian/Desktop/Data-Science-Fundamentals/lecture_11/worksheet_11.ipynb#W1sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m vectorizer \u001b[39m=\u001b[39m TfidfVectorizer(stop_words\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m, min_df\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m,max_df\u001b[39m=\u001b[39m\u001b[39m0.8\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/thian/Desktop/Data-Science-Fundamentals/lecture_11/worksheet_11.ipynb#W1sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m stemmed_data \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(SnowballStemmer(\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m, ignore_stopwords\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mstem(word)  \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thian/Desktop/Data-Science-Fundamentals/lecture_11/worksheet_11.ipynb#W1sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m          \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sent_tokenize(message)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thian/Desktop/Data-Science-Fundamentals/lecture_11/worksheet_11.ipynb#W1sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m         \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m word_tokenize(sent))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thian/Desktop/Data-Science-Fundamentals/lecture_11/worksheet_11.ipynb#W1sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m         \u001b[39mfor\u001b[39;00m message \u001b[39min\u001b[39;00m news_data\u001b[39m.\u001b[39mdata]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thian/Desktop/Data-Science-Fundamentals/lecture_11/worksheet_11.ipynb#W1sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m dtm \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39mfit_transform(stemmed_data)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thian/Desktop/Data-Science-Fundamentals/lecture_11/worksheet_11.ipynb#W1sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m terms \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39mget_feature_names_out()\n",
      "\u001b[1;32m/Users/thian/Desktop/Data-Science-Fundamentals/lecture_11/worksheet_11.ipynb Cell 2\u001b[0m line \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thian/Desktop/Data-Science-Fundamentals/lecture_11/worksheet_11.ipynb#W1sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m news_data \u001b[39m=\u001b[39m fetch_20newsgroups(subset\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m, categories\u001b[39m=\u001b[39mcategories)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thian/Desktop/Data-Science-Fundamentals/lecture_11/worksheet_11.ipynb#W1sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m vectorizer \u001b[39m=\u001b[39m TfidfVectorizer(stop_words\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m, min_df\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m,max_df\u001b[39m=\u001b[39m\u001b[39m0.8\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thian/Desktop/Data-Science-Fundamentals/lecture_11/worksheet_11.ipynb#W1sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m stemmed_data \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(SnowballStemmer(\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m, ignore_stopwords\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mstem(word)  \n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/thian/Desktop/Data-Science-Fundamentals/lecture_11/worksheet_11.ipynb#W1sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m          \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sent_tokenize(message)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thian/Desktop/Data-Science-Fundamentals/lecture_11/worksheet_11.ipynb#W1sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m         \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m word_tokenize(sent))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thian/Desktop/Data-Science-Fundamentals/lecture_11/worksheet_11.ipynb#W1sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m         \u001b[39mfor\u001b[39;00m message \u001b[39min\u001b[39;00m news_data\u001b[39m.\u001b[39mdata]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thian/Desktop/Data-Science-Fundamentals/lecture_11/worksheet_11.ipynb#W1sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m dtm \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39mfit_transform(stemmed_data)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thian/Desktop/Data-Science-Fundamentals/lecture_11/worksheet_11.ipynb#W1sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m terms \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39mget_feature_names_out()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msent_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[39m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[39m=\u001b[39m load(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtokenizers/punkt/\u001b[39;49m\u001b[39m{\u001b[39;49;00mlanguage\u001b[39m}\u001b[39;49;00m\u001b[39m.pickle\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    107\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m<<Loading \u001b[39m\u001b[39m{\u001b[39;00mresource_url\u001b[39m}\u001b[39;00m\u001b[39m>>\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[39m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[39m=\u001b[39m _open(resource_url)\n\u001b[1;32m    752\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mformat\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[39m=\u001b[39m opened_resource\u001b[39m.\u001b[39mread()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[39m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[39mif\u001b[39;00m protocol \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnltk\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, path \u001b[39m+\u001b[39;49m [\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m])\u001b[39m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[39melif\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[39m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, [\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mopen()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/thian/nltk_data'\n    - '/Users/thian/opt/anaconda3/nltk_data'\n    - '/Users/thian/opt/anaconda3/share/nltk_data'\n    - '/Users/thian/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "categories = ['comp.os.ms-windows.misc', 'sci.space','rec.sport.baseball']\n",
    "news_data = fetch_20newsgroups(subset='train', categories=categories)\n",
    "vectorizer = TfidfVectorizer(stop_words='english', min_df=4,max_df=0.8)\n",
    "\n",
    "stemmed_data = [\" \".join(SnowballStemmer(\"english\", ignore_stopwords=True).stem(word)  \n",
    "         for sent in sent_tokenize(message)\n",
    "        for word in word_tokenize(sent))\n",
    "        for message in news_data.data]\n",
    "\n",
    "dtm = vectorizer.fit_transform(stemmed_data)\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "centered_dtm = dtm - np.mean(dtm, axis=0)\n",
    "\n",
    "u, s, vt = np.linalg.svd(centered_dtm)\n",
    "plt.xlim([0,50])\n",
    "plt.plot(range(1,len(s)+1),s)\n",
    "plt.show()\n",
    "\n",
    "ag = []\n",
    "max = len(u)\n",
    "for k in range(1,25):\n",
    "    vectorsk = u.dot(np.diag(s))[:,:k]\n",
    "    kmeans = KMeans(n_clusters=3, init='k-means++', max_iter=100, n_init=10, random_state=0)\n",
    "    kmeans.fit_predict(np.asarray(vectorsk))\n",
    "    labelsk = kmeans.labels_\n",
    "    ag.append(metrics.v_measure_score(labelsk, news_data.target)) # closer to 1 means closer to news categories\n",
    "\n",
    "plt.plot(range(1,25),ag)\n",
    "plt.ylabel('Agreement',size=20)\n",
    "plt.xlabel('No of Prin Comps',size=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "\n",
    "The data comes from the [Yelp Dataset](https://www.yelp.com/dataset). Each line is a review that consists of a label (0 for negative reviews and 1 for positive reviews) and a set of words.\n",
    "\n",
    "```\n",
    "1 i will never forget this single breakfast experience in mad...\n",
    "0 the search for decent chinese takeout in madison continues ...\n",
    "0 sorry but me julio fell way below the standard even for med...\n",
    "1 so this is the kind of food that will kill you so there s t...\n",
    "```\n",
    "\n",
    "In order to transform the set of words into vectors, we will rely on a method of feature engineering called word embeddings (Tfidf is one way to get these embeddings). Rather than simply indicating which words are present, word embeddings represent each word by \"embedding\" it in a low-dimensional vector space which may carry more information about the semantic meaning of the word. (for example in this space, the words \"King\" and \"Queen\" would be close).\n",
    "\n",
    "`word2vec.txt` contains the `word2vec` embeddings for about 15 thousand words. Not every word in each review is present in the provided `word2vec.txt` file. We can treat these words as being \"out of vocabulary\" and ignore them.\n",
    "\n",
    "### Example\n",
    "\n",
    "Let x_i denote the sentence `“a hot dog is not a sandwich because it is not square”` and let a toy word2vec dictionary be as follows:\n",
    "\n",
    "```\n",
    "hot      0.1     0.2     0.3\n",
    "not      -0.1    0.2     -0.3\n",
    "sandwich 0.0     -0.2    0.4\n",
    "square   0.2     -0.1    0.5\n",
    "```\n",
    "\n",
    "we would first `trim` the sentence to only contain words in our vocabulary: `\"hot not sandwich not square”` then embed x_i into the feature space:\n",
    "\n",
    "$$ φ2(x_i)) = \\frac{1}{5} (word2vec(\\text{hot}) + 2 · word2vec(\\text{not}) + word2vec(\\text{sandwich}) + word2vec(\\text{square})) = \\left[0.02 \\hspace{2mm} 0.06 \\hspace{2mm} 0.12 \\hspace{2mm}\\right]^T $$\n",
    "\n",
    "a) Implement a function to trim out-of-vocabulary words from the reviews. Your function should return an nd array of the same dimension and dtype as the original loaded dataset. (10pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "./data/train_small.tsv not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/Users/thian/Desktop/Data-Science-Fundamentals/lecture_11/worksheet_11.ipynb Cell 4\u001b[0m line \u001b[0;36m<cell line: 59>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thian/Desktop/Data-Science-Fundamentals/lecture_11/worksheet_11.ipynb#W3sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m         trimmed_dataset\u001b[39m.\u001b[39mappend((label, trimmed_review))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thian/Desktop/Data-Science-Fundamentals/lecture_11/worksheet_11.ipynb#W3sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(trimmed_dataset, dtype\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mO\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/thian/Desktop/Data-Science-Fundamentals/lecture_11/worksheet_11.ipynb#W3sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m trim_train \u001b[39m=\u001b[39m trim_reviews(\u001b[39m\"\u001b[39;49m\u001b[39m./data/train_small.tsv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thian/Desktop/Data-Science-Fundamentals/lecture_11/worksheet_11.ipynb#W3sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m trim_test \u001b[39m=\u001b[39m trim_reviews(\u001b[39m\"\u001b[39m\u001b[39m./data/test_small.tsv\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/thian/Desktop/Data-Science-Fundamentals/lecture_11/worksheet_11.ipynb Cell 4\u001b[0m line \u001b[0;36mtrim_reviews\u001b[0;34m(path_to_dataset)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thian/Desktop/Data-Science-Fundamentals/lecture_11/worksheet_11.ipynb#W3sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrim_reviews\u001b[39m(path_to_dataset):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/thian/Desktop/Data-Science-Fundamentals/lecture_11/worksheet_11.ipynb#W3sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     original_dataset \u001b[39m=\u001b[39m load_tsv_dataset(path_to_dataset)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thian/Desktop/Data-Science-Fundamentals/lecture_11/worksheet_11.ipynb#W3sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     trimmed_dataset \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thian/Desktop/Data-Science-Fundamentals/lecture_11/worksheet_11.ipynb#W3sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     \u001b[39mfor\u001b[39;00m data_point \u001b[39min\u001b[39;00m original_dataset:\n",
      "\u001b[1;32m/Users/thian/Desktop/Data-Science-Fundamentals/lecture_11/worksheet_11.ipynb Cell 4\u001b[0m line \u001b[0;36mload_tsv_dataset\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/thian/Desktop/Data-Science-Fundamentals/lecture_11/worksheet_11.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_tsv_dataset\u001b[39m(file):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/thian/Desktop/Data-Science-Fundamentals/lecture_11/worksheet_11.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/thian/Desktop/Data-Science-Fundamentals/lecture_11/worksheet_11.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m    Loads raw data and returns a tuple containing the reviews and their ratings.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thian/Desktop/Data-Science-Fundamentals/lecture_11/worksheet_11.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thian/Desktop/Data-Science-Fundamentals/lecture_11/worksheet_11.ipynb#W3sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m        an integer (0 or 1) and the review is a string.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thian/Desktop/Data-Science-Fundamentals/lecture_11/worksheet_11.ipynb#W3sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/thian/Desktop/Data-Science-Fundamentals/lecture_11/worksheet_11.ipynb#W3sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     dataset \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mloadtxt(file, delimiter\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m'\u001b[39;49m, comments\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, encoding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thian/Desktop/Data-Science-Fundamentals/lecture_11/worksheet_11.ipynb#W3sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m                          dtype\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39ml,O\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thian/Desktop/Data-Science-Fundamentals/lecture_11/worksheet_11.ipynb#W3sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m dataset\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numpy/lib/npyio.py:1067\u001b[0m, in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, like)\u001b[0m\n\u001b[1;32m   1065\u001b[0m     fname \u001b[39m=\u001b[39m os_fspath(fname)\n\u001b[1;32m   1066\u001b[0m \u001b[39mif\u001b[39;00m _is_string_like(fname):\n\u001b[0;32m-> 1067\u001b[0m     fh \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mlib\u001b[39m.\u001b[39;49m_datasource\u001b[39m.\u001b[39;49mopen(fname, \u001b[39m'\u001b[39;49m\u001b[39mrt\u001b[39;49m\u001b[39m'\u001b[39;49m, encoding\u001b[39m=\u001b[39;49mencoding)\n\u001b[1;32m   1068\u001b[0m     fencoding \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(fh, \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlatin1\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   1069\u001b[0m     fh \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(fh)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numpy/lib/_datasource.py:193\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[39mOpen `path` with `mode` and return the file object.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    189\u001b[0m \n\u001b[1;32m    190\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    192\u001b[0m ds \u001b[39m=\u001b[39m DataSource(destpath)\n\u001b[0;32m--> 193\u001b[0m \u001b[39mreturn\u001b[39;00m ds\u001b[39m.\u001b[39;49mopen(path, mode, encoding\u001b[39m=\u001b[39;49mencoding, newline\u001b[39m=\u001b[39;49mnewline)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numpy/lib/_datasource.py:533\u001b[0m, in \u001b[0;36mDataSource.open\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[39mreturn\u001b[39;00m _file_openers[ext](found, mode\u001b[39m=\u001b[39mmode,\n\u001b[1;32m    531\u001b[0m                               encoding\u001b[39m=\u001b[39mencoding, newline\u001b[39m=\u001b[39mnewline)\n\u001b[1;32m    532\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 533\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m not found.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m path)\n",
      "\u001b[0;31mOSError\u001b[0m: ./data/train_small.tsv not found."
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "VECTOR_LEN = 300   # Length of word2vec vector\n",
    "MAX_WORD_LEN = 64  # Max word length in dict.txt and word2vec.txt\n",
    "\n",
    "def load_tsv_dataset(file):\n",
    "    \"\"\"\n",
    "    Loads raw data and returns a tuple containing the reviews and their ratings.\n",
    "\n",
    "    Parameters:\n",
    "        file (str): File path to the dataset tsv file.\n",
    "\n",
    "    Returns:\n",
    "        An np.ndarray of shape N. N is the number of data points in the tsv file.\n",
    "        Each element dataset[i] is a tuple (label, review), where the label is\n",
    "        an integer (0 or 1) and the review is a string.\n",
    "    \"\"\"\n",
    "    dataset = np.loadtxt(file, delimiter='\\t', comments=None, encoding='utf-8',\n",
    "                         dtype='l,O')\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def load_feature_dictionary(file):\n",
    "    \"\"\"\n",
    "    Creates a map of words to vectors using the file that has the word2vec\n",
    "    embeddings.\n",
    "\n",
    "    Parameters:\n",
    "        file (str): File path to the word2vec embedding file.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary indexed by words, returning the corresponding word2vec\n",
    "        embedding np.ndarray.\n",
    "    \"\"\"\n",
    "    word2vec_map = dict()\n",
    "    with open(file) as f:\n",
    "        read_file = csv.reader(f, delimiter='\\t')\n",
    "        for row in read_file:\n",
    "            word, embedding = row[0], row[1:]\n",
    "            word2vec_map[word] = np.array(embedding, dtype=float)\n",
    "    return word2vec_map\n",
    "\n",
    "\n",
    "def trim_reviews(path_to_dataset):\n",
    "    original_dataset = load_tsv_dataset(path_to_dataset)\n",
    "    \n",
    "    trimmed_dataset = []\n",
    "\n",
    "    for data_point in original_dataset:\n",
    "        label, review = data_point\n",
    "        words = review.split()\n",
    "        trimmed_review = ' '.join([word for word in words if word in word2vec_dict])\n",
    "\n",
    "        trimmed_dataset.append((label, trimmed_review))\n",
    "\n",
    "    return np.array(trimmed_dataset, dtype='O')\n",
    "\n",
    "trim_train = trim_reviews(\"./data/train_small.tsv\")\n",
    "trim_test = trim_reviews(\"./data/test_small.tsv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "76ca05dc3ea24b2e3b98cdb7774adfbb40773424bf5109b477fd793f623715af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
